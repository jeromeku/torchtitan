+ NGPU=8
+ NNODES=1
+ CONFIG_FILE=./torchtitan/models/llama3/train_configs/debug_model.toml
+ overrides=
+ '[' 0 -ne 0 ']'
+ export WORLD_SIZE=8
+ WORLD_SIZE=8
+ export LOCAL_RANK=0
+ LOCAL_RANK=0
+ python -m scripts.estimate.estimation --job.config_file ./torchtitan/models/llama3/train_configs/debug_model.toml --memory_estimation.enabled
[titan] 2025-07-16 13:01:47,290 - root - INFO - Estimating memory usage...
[titan] 2025-07-16 13:01:47,290 - root - INFO - Building 1-D device mesh with ['dp_shard'], [8]
[titan] 2025-07-16 13:01:47,535 - root - INFO - Loading tokenizer from tokenizer.json
[titan] 2025-07-16 13:01:47,537 - root - INFO - Building llama3 debugmodel with TransformerModelArgs(_enforced='This field is used to enforce all fields have defaults.', dim=256, n_layers=3, n_heads=16, n_kv_heads=None, vocab_size=2000, multiple_of=256, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=2048, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=1999)
[titan] 2025-07-16 13:01:47,559 - root - INFO - Applied selective activation checkpointing to the model
[titan] 2025-07-16 13:01:47,575 - root - INFO - Applied FSDP to the model
[titan] 2025-07-16 13:01:47,728 - root - INFO - Vocab size: 2000
sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.
comms_counts=defaultdict(<class 'int'>, {<OpOverloadPacket(op='c10d._allgather_base_')>: 7, <OpOverloadPacket(op='c10d._reduce_scatter_base_')>: 4, <OpOverloadPacket(op='c10d_functional.all_reduce')>: 1})
Global
  FORWARD PASS
[1;33m    *c10d._allgather_base_: 4[0m
[1;33m    *c10d_functional.all_reduce: 1[0m
  BACKWARD PASS
[1;33m    *c10d._allgather_base_: 3[0m
[1;33m    *c10d._reduce_scatter_base_: 4[0m
    FSDPTransformer
    *module type: class 'abc.FSDPTransformer'
      FORWARD PASS
[1;33m        *c10d._allgather_base_: 4[0m
[1;33m        *c10d_functional.all_reduce: 1[0m
      BACKWARD PASS
[1;33m        *c10d._allgather_base_: 3[0m
[1;33m        *c10d._reduce_scatter_base_: 4[0m
        FSDPTransformer.tok_embeddings
        *module type: class 'torch.nn.modules.sparse.Embedding'
          FORWARD PASS
[1;33m            *c10d_functional.all_reduce: 1[0m
[1;33m            **aten.linalg_vector_norm.default[0m
[1;31m              shape: [torch.Size([30])][0m
[1;31m              sharding: [(_NormPartial(reduce_op='sum', norm_type=2.0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.add.Tensor[0m
[1;31m              shape: [torch.Size([])][0m
[1;31m              sharding: [(_NormPartial(reduce_op='sum', norm_type=2.0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.reciprocal.default[0m
[1;31m              shape: [torch.Size([])][0m
[1;31m              sharding: [(_NormPartial(reduce_op='sum', norm_type=2.0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.mul.Tensor[0m
[1;31m              shape: [torch.Size([])][0m
[1;31m              sharding: [(Replicate(),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.clamp.default[0m
[1;31m              shape: [torch.Size([])][0m
[1;31m              sharding: [(Replicate(),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten._foreach_mul_.Tensor[0m
[1;31m              shape: [torch.Size([])][0m
[1;31m              sharding: [(Replicate(),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([2000, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([2000, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([768, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([768, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 768])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 768])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([768, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([768, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([768, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([768, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 768])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 768])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([768, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([768, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([768, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([768, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 768])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256, 768])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([768, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([768, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([2000, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
[1;33m            **aten.zeros_like.default[0m
[1;31m              shape: [torch.Size([2000, 256])][0m
[1;31m              sharding: [(Shard(dim=0),)][0m
[1;31m              device mesh: DeviceMesh('cuda', [0, 1, 2, 3, 4, 5, 6, 7], mesh_dim_names=('dp_shard_cp',))[0m
          BACKWARD PASS
[1;33m            *c10d._reduce_scatter_base_: 1[0m
        FSDPTransformer.layers.0
        *module type: class 'torch.distributed.fsdp._fully_shard._fully_shard.FSDPTransformerBlock'
          FORWARD PASS
[1;33m            *c10d._allgather_base_: 1[0m
          BACKWARD PASS
[1;33m            *c10d._reduce_scatter_base_: 1[0m
            FSDPTransformer.layers.0.attention_norm
            *module type: class 'torch.nn.modules.normalization.RMSNorm'
              FORWARD PASS
              BACKWARD PASS
[1;33m                *c10d._reduce_scatter_base_: 1[0m
            FSDPTransformer.layers.0.attention
            *module type: class 'torchtitan.models.llama3.model.model.Attention'
              FORWARD PASS
                FSDPTransformer.layers.0.attention.wq
                *module type: class 'torch.nn.modules.linear.Linear'
                  FORWARD PASS
                  BACKWARD PASS
                FSDPTransformer.layers.0.attention.wk
                *module type: class 'torch.nn.modules.linear.Linear'
                  FORWARD PASS
                  BACKWARD PASS
                FSDPTransformer.layers.0.attention.wv
                *module type: class 'torch.nn.modules.linear.Linear'
                  FORWARD PASS
                  BACKWARD PASS
                FSDPTransformer.layers.0.attention.sdpa
                *module type: class 'torchtitan.models.attention.ScaledDotProductAttention'
                  FORWARD PASS
                  BACKWARD PASS
                FSDPTransformer.layers.0.attention.wo
                *module type: class 'torch.nn.modules.linear.Linear'
                  FORWARD PASS
                  BACKWARD PASS
            FSDPTransformer.layers.0.ffn_norm
            *module type: class 'torch.nn.modules.normalization.RMSNorm'
              FORWARD PASS
              BACKWARD PASS
            FSDPTransformer.layers.0.feed_forward
            *module type: class 'torchtitan.models.llama3.model.model.FeedForward'
              FORWARD PASS
                FSDPTransformer.layers.0.feed_forward.w1
                *module type: class 'torch.nn.modules.linear.Linear'
                  FORWARD PASS
                  BACKWARD PASS
                FSDPTransformer.layers.0.feed_forward.w3
                *module type: class 'torch.nn.modules.linear.Linear'
                  FORWARD PASS
                  BACKWARD PASS
                FSDPTransformer.layers.0.feed_forward.w2
                *module type: class 'torch.nn.modules.linear.Linear'
                  FORWARD PASS
                  BACKWARD PASS
        FSDPTransformer.layers.1
        *module type: class 'abc.FSDPCheckpointWrapper'
          FORWARD PASS
[1;33m            *c10d._allgather_base_: 1[0m
          BACKWARD PASS
[1;33m            *c10d._allgather_base_: 1[0m
[1;33m            *c10d._reduce_scatter_base_: 1[0m
            FSDPTransformer.layers.1._checkpoint_wrapped_module
            *module type: class 'torchtitan.models.llama3.model.model.TransformerBlock'
              FORWARD PASS
              BACKWARD PASS
[1;33m                *c10d._reduce_scatter_base_: 1[0m
              ACTIVATION CHECKPOINTING
                FSDPTransformer.layers.1._checkpoint_wrapped_module.attention_norm
                *module type: class 'torch.nn.modules.normalization.RMSNorm'
                  FORWARD PASS
                  BACKWARD PASS
[1;33m                    *c10d._reduce_scatter_base_: 1[0m
                  ACTIVATION CHECKPOINTING
                FSDPTransformer.layers.1._checkpoint_wrapped_module.attention
                *module type: class 'torchtitan.models.llama3.model.model.Attention'
                  FORWARD PASS
                  ACTIVATION CHECKPOINTING
                    FSDPTransformer.layers.1._checkpoint_wrapped_module.attention.wq
                    *module type: class 'torch.nn.modules.linear.Linear'
                      FORWARD PASS
                      BACKWARD PASS
                      ACTIVATION CHECKPOINTING
                    FSDPTransformer.layers.1._checkpoint_wrapped_module.attention.wk
                    *module type: class 'torch.nn.modules.linear.Linear'
                      FORWARD PASS
                      BACKWARD PASS
                      ACTIVATION CHECKPOINTING
                    FSDPTransformer.layers.1._checkpoint_wrapped_module.attention.wv
                    *module type: class 'torch.nn.modules.linear.Linear'
                      FORWARD PASS
                      BACKWARD PASS
                      ACTIVATION CHECKPOINTING
                    FSDPTransformer.layers.1._checkpoint_wrapped_module.attention.sdpa
                    *module type: class 'torchtitan.models.attention.ScaledDotProductAttention'
                      FORWARD PASS
                      BACKWARD PASS
                      ACTIVATION CHECKPOINTING
                    FSDPTransformer.layers.1._checkpoint_wrapped_module.attention.wo
                    *module type: class 'torch.nn.modules.linear.Linear'
                      FORWARD PASS
                      BACKWARD PASS
                      ACTIVATION CHECKPOINTING
                FSDPTransformer.layers.1._checkpoint_wrapped_module.ffn_norm
                *module type: class 'torch.nn.modules.normalization.RMSNorm'
                  FORWARD PASS
                  BACKWARD PASS
                  ACTIVATION CHECKPOINTING
                FSDPTransformer.layers.1._checkpoint_wrapped_module.feed_forward
                *module type: class 'torchtitan.models.llama3.model.model.FeedForward'
                  FORWARD PASS
                  ACTIVATION CHECKPOINTING
                    FSDPTransformer.layers.1._checkpoint_wrapped_module.feed_forward.w1
                    *module type: class 'torch.nn.modules.linear.Linear'
                      FORWARD PASS
                      BACKWARD PASS
                      ACTIVATION CHECKPOINTING
                    FSDPTransformer.layers.1._checkpoint_wrapped_module.feed_forward.w3
                    *module type: class 'torch.nn.modules.linear.Linear'
                      FORWARD PASS
                      BACKWARD PASS
                      ACTIVATION CHECKPOINTING
                    FSDPTransformer.layers.1._checkpoint_wrapped_module.feed_forward.w2
                    *module type: class 'torch.nn.modules.linear.Linear'
                      FORWARD PASS
                      BACKWARD PASS
                      ACTIVATION CHECKPOINTING
        FSDPTransformer.layers.2
        *module type: class 'torch.distributed.fsdp._fully_shard._fully_shard.FSDPTransformerBlock'
          FORWARD PASS
[1;33m            *c10d._allgather_base_: 1[0m
          BACKWARD PASS
[1;33m            *c10d._allgather_base_: 1[0m
[1;33m            *c10d._reduce_scatter_base_: 1[0m
            FSDPTransformer.layers.2.attention_norm
            *module type: class 'torch.nn.modules.normalization.RMSNorm'
              FORWARD PASS
              BACKWARD PASS
[1;33m                *c10d._reduce_scatter_base_: 1[0m
            FSDPTransformer.layers.2.attention
            *module type: class 'torchtitan.models.llama3.model.model.Attention'
              FORWARD PASS
                FSDPTransformer.layers.2.attention.wq
                *module type: class 'torch.nn.modules.linear.Linear'
                  FORWARD PASS
                  BACKWARD PASS
                FSDPTransformer.layers.2.attention.wk
                *module type: class 'torch.nn.modules.linear.Linear'
                  FORWARD PASS
                  BACKWARD PASS
                FSDPTransformer.layers.2.attention.wv
                *module type: class 'torch.nn.modules.linear.Linear'
                  FORWARD PASS
                  BACKWARD PASS
                FSDPTransformer.layers.2.attention.sdpa
                *module type: class 'torchtitan.models.attention.ScaledDotProductAttention'
                  FORWARD PASS
                  BACKWARD PASS
                FSDPTransformer.layers.2.attention.wo
                *module type: class 'torch.nn.modules.linear.Linear'
                  FORWARD PASS
                  BACKWARD PASS
            FSDPTransformer.layers.2.ffn_norm
            *module type: class 'torch.nn.modules.normalization.RMSNorm'
              FORWARD PASS
              BACKWARD PASS
            FSDPTransformer.layers.2.feed_forward
            *module type: class 'torchtitan.models.llama3.model.model.FeedForward'
              FORWARD PASS
                FSDPTransformer.layers.2.feed_forward.w1
                *module type: class 'torch.nn.modules.linear.Linear'
                  FORWARD PASS
                  BACKWARD PASS
                FSDPTransformer.layers.2.feed_forward.w3
                *module type: class 'torch.nn.modules.linear.Linear'
                  FORWARD PASS
                  BACKWARD PASS
                FSDPTransformer.layers.2.feed_forward.w2
                *module type: class 'torch.nn.modules.linear.Linear'
                  FORWARD PASS
                  BACKWARD PASS
        FSDPTransformer.norm
        *module type: class 'torch.nn.modules.normalization.RMSNorm'
          FORWARD PASS
          BACKWARD PASS
        FSDPTransformer.output
        *module type: class 'torch.nn.modules.linear.Linear'
          FORWARD PASS
          BACKWARD PASS

